# Краулер веб-сайтов 

Это полнофункциональный асинхронный веб-краулер с многопользовательским веб-интерфейсом, построенный на Flask. Система позволяет пользователям создавать и управлять задачами по сбору данных с веб-сайтов, отслеживать их выполнение в реальном времени и экспортировать результаты в структурированном формате.

## Скриншоты


| Панель управления | Создание задачи | Результаты |
| :---: |:---:|:---:|
| *![Панель управления](static/img/dashboard.png)* | *![Создание задачи](static/img/create_job.png)* | *![Результаты](static/img/job_details.png)* |

## Ключевые возможности

- **Асинхронное ядро**: Высокопроизводительный сбор данных с использованием `aiohttp` и `asyncio`.
- **Многопользовательская система**: Регистрация, аутентификация и система ролей (пользователь, администратор).
- **Уважение к правилам**: Автоматическая проверка и соблюдение `robots.txt`.
- **Гибкая настройка задач**: Управление глубиной обхода, максимальным количеством страниц и задержкой между запросами.
- **Интерактивный веб-интерфейс**: Панель управления для мониторинга задач, созданная с помощью Flask и Bootstrap 5.
- **Отслеживание в реальном времени**: Мониторинг прогресса выполнения задач без перезагрузки страницы.
- **Надежное хранилище**: Использование PostgreSQL с гибким типом данных `JSONB` для хранения собранной информации.
- **Экспорт данных**: Возможность выгрузить все собранные по задаче данные в `JSON` файл.
- **Административная панель**: Управление пользователями и просмотр всех задач в системе.

## Технологический стек

- **Backend**: Python 3.10+, Flask
- **Асинхронность**: `aiohttp`, `asyncio`
- **Парсинг**: `BeautifulSoup4`
- **База данных**: PostgreSQL
- **Frontend**: HTML, CSS, JavaScript, Bootstrap 5, Jinja2
- **Безопасность**: `bcrypt` для хеширования паролей

## Архитектура системы

Приложение состоит из трех основных компонентов, работающих вместе:

1.  **Веб-приложение (Flask)**: Обрабатывает HTTP-запросы от пользователей, управляет сессиями, аутентификацией и отображает интерфейс.
2.  **Асинхронный краулер (aiohttp)**: Запускается в отдельном потоке для каждой задачи. Выполняет асинхронный сбор и парсинг веб-страниц.
3.  **База данных (PostgreSQL)**: Хранит информацию о пользователях, задачах, собранных страницах и ссылках.

```mermaid
graph TD
    A[Пользователь] -->|HTTP-запросы| B(Веб-приложение Flask);
    B -->|Запуск в отдельном потоке| C{Асинхронный краулер};
    B <-->|CRUD операции| D[(PostgreSQL База данных)];
    C -->|Сохранение данных| D;
    C -->|HTTP-запросы| E(Целевые веб-сайты);
```

## ER-диаграмма базы данных

![ER-диаграмма](static/img/er-diagram.png)



## Установка и запуск

### 1. Предварительные требования
- Python 3.10+
- Git
- PostgreSQL

### 2. Клонирование репозитория
```bash
git clone <URL-вашего-репозитория>
cd web-crawler/Crawler
```

### 3. Настройка виртуального окружения и зависимостей
```bash
# Создание виртуального окружения
python -m venv venv

# Активация (для Windows)
venv\Scripts\activate

# Активация (для macOS/Linux)
source venv/bin/activate

# Установка зависимостей
pip install -r requirements.txt
```

### 4. Настройка базы данных PostgreSQL
1.  Создайте новую базу данных и пользователя в PostgreSQL.
2.  Создайте таблицы согласно вашей ER-диаграмме (см. выше).

### 5. Конфигурация
Приложение использует файл `config.py` для настроек. Для подключения к БД убедитесь, что переменная окружения `DATABASE_URL` установлена в формате:
`postgresql://USER:PASSWORD@HOST:PORT/DATABASE`

Также установите `SECRET_KEY` для Flask-сессий.

### 6. Запуск приложения
```bash
python app.py
```
Приложение будет доступно по адресу `http://127.0.0.1:5000`.

## Использование


Базовый пример использования:

```python
from crawler import WebCrawler
import asyncio

async def main():
    crawler = WebCrawler(
        start_url="https://example.com",
        max_pages=100,      # Максимальное количество страниц
        delay=0.5,         # Задержка между запросами
        max_depth=3,       # Максимальная глубина обхода
        max_retries=3,     # Количество попыток при ошибках
        user_agent="MyCrawler/1.0"  # Идентификатор краулера
    )
    await crawler.crawl()

if __name__ == "__main__":
    asyncio.run(main())
```

## Параметры конфигурации

- `start_url` (str): Начальный URL для обхода
- `max_pages` (int): Максимальное количество страниц для обхода (по умолчанию: 100)
- `delay` (float): Задержка между запросами в секундах (по умолчанию: 1.0)
- `max_depth` (int): Максимальная глубина обхода (по умолчанию: 3)
- `max_retries` (int): Количество попыток при ошибках (по умолчанию: 3)
- `user_agent` (str): User-Agent для запросов (по умолчанию: "*")

## Собранные данные

Краулер собирает следующие данные с каждой страницы:

- URL страницы
- Заголовок страницы
- Мета-описание
- Ключевые слова
- Open Graph теги
- Заголовки (h1-h6)
- Основной текст
- Количество слов
- Количество ссылок
- Список ссылок
- Временная метка



1.  **Регистрация**: Новые пользователи могут зарегистрироваться через форму на главной странице.
2.  **Создание задачи**: На панели управления нажмите "Создать задачу", введите название, стартовый URL и настройте параметры обхода.
3.  **Просмотр результатов**: После завершения задачи перейдите на страницу деталей для просмотра статистики, списка страниц и собранных данных.
4.  **Экспорт**: На странице деталей задачи можно выгрузить все данные в JSON-файл.

## Структура проекта

```
Crawler/
├── app.py              # Основной файл Flask-приложения (маршруты, контроллеры)
├── crawler.py          # Ядро асинхронного краулера
├── database.py         # Менеджер для работы с базой данных PostgreSQL
├── config.py           # Конфигурация приложения
├── requirements.txt    # Список зависимостей
├── static/             # Статические файлы (CSS, JS)
└── templates/          # HTML-шаблоны (Jinja2)
```


